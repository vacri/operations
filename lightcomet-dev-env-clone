#!/usr/bin/env python3

""" Lightcomet is a product where the staging and testing envs were
    reset with production data (db and web assets) each day, and
    this is the script that did that resetting, called by cron

    Given an s3 location, pull the sql dumps starting with `lc_` and
    restore them to staging and testing databases. Fish the various
    settings out of AWS Parameter Store, and skip restoration if
    there are no associated creds there.  In addition, do a clone
    from the production asset bucket to the other env asset buckets

    This script is adapted from the more generic rds-db-tester,
    which finds sql dumps and tries to restore them against the same
    temporary test db. The workflow in this script is a little
    hairier, because instead of "restore this dump to a single
    temporary database" it needs "find both the stag and test db
    that go with this prod db, find the individual credentials for
    that db, and restore it"

    If I was to do this script from scratch, I'd use a config file
    rather than overloading the CLI args... damn it's messy and
    brittle
"""

import sys
import os
import argparse
import textwrap
import logging
import time
## add this line if we start using common in-house libs
#sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), 'lib'))
from urllib.parse import urlparse
from datetime import datetime, timedelta, timezone
import subprocess
import tempfile
import requests
import boto3
import mysql.connector
try:
    from slackclient import SlackClient
except:
    # 'slackclient' installs a different module name in v2+
    # and has a different workflow
    import slack
import pkg_resources



log = logging.getLogger(os.path.basename(sys.argv[0]))
log.setLevel(logging.DEBUG)


def getoptions():
    """ ... gets... options?
    """

    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent('''

            Usage:
              lightcomet-dev-env-clone s3://mybucket/path/to/sqldumps/

            lightcomet-database-clone queries the nominated location for sql
            dumpfiles that start with the nominated filename prefix

            From these filenames, namespaces for each database are determined,
            then appropriate connection parameters are looked up in
            AWS Parameter Store. The script will then attempt to restore the
            database to the nominated mysql host. At time of writing, both
            staging + testing databases use  the same host, so it is hardcoded

            THE NAMING SYSTEM is very brittle - the dump filenamess on s3 MUST
            be in the format PREFIX_NAMESPACE_ENV.sql (lc_clientname_prod.sql),
            as all of these fields are extracted and used to programmatically
            determing which Parameter Store fields to look up and what the
            target databases are. The ENV must be four characters long
            (test/stag/prod etc) and it's assumed that there is only one dump
            per namespace in the target directory (ie: not lc_clientname_stag
            and lc_clientname_test in the same dir).

            SQL dumps older than 2 days (default) will also be ignored, with
            the assumption that live databases are backed up more often than
            this.

            There is an exclusion list where you can put target database names
            to be ignored, allowing the db clone to be skipped for that
            database. This is of use if developers don't want to reset a
            particular database during the job run (eg: persist it overnight).
            The exclusion list is simply a list of database names as they
            appear in mysql 'show databases' - if the name is in there, it will
            be skipped.

            By default, only the staging environment will be cloned. The idea
            here is that the staging environment will be updated to look like
            prod, and the testing environment is left alone and will keep
            changes from day to day.

            Requirements:
            - SQL details in AWS Parameter Store for each db
                - creds must be able to create/drop/restore the 'dbname'
                - /LONG_ENV/rds/RDS_INSTANCE_NAME/DBNAME/{user,pass}
                  - LONG_ENV = 'staging', not 'stag'
                  - RDS_INSTANCE_NAME = name of instance in RDS list
                     (the first subdomain of the FQDN)
                - /admin/lightcomet/nightly-clone/slack-notify-token and
                     /slack-notify-channel
            - disk space to temporarily hold dumps
            - aws tools
            - slack tools
            - network access to target Mysql hosts
            - IAM write access to target S3 asset buckets
            - IAM read access to source sql dumps, Parameter Store items,
              and source s3 asset bucket

            Limitations:
            - Only has MySQL support, no postgres
            - no support for compressed dumps
            - s3 pagination not implemented for mysql dumps - don't use
              for locations with more than 999 files (any kind, not just .sql) :)
            - assumes that each sql dumpfile has a unique basename

        '''))

    parser.add_argument('s3_loc',
                        help='Full S3 path to location holding sql files for restore',
                        default='help'
                       )
    parser.add_argument('-r', '--region', help='', default='ap-southeast-2')
    parser.add_argument('-e', '--engine', help='sql engine', default='mysql')
    parser.add_argument('-d', '--dryrun', help='run sync in dry-run mode', action='store_true')
    parser.add_argument('-l', '--loglevel', help='', default='INFO')
    parser.add_argument('-m', '--max-age', help='max age of dumpfiles to be cloned. Older files assumed to be abandoned services', default=2)
    parser.add_argument('-s', '--suppress-notifications',
                        help='block updates to slack, prometheus',
                        action='store_true'
                       )
    parser.add_argument('--exclusion-list',
                        help='file with a simple list of db names to exclude, where the db names are as they appear in `show databases;` in mysql',
                        default='/etc/lightcomet-dev-env-clone.exclusions'
                       )
    parser.add_argument('--database-prefix',
                        help='only use sql dumpfiles with this prefix',
                        default='lc_'
                       )
    parser.add_argument('--environments',
                        help='comma-linked list of environments to clone to, as they appear in AWS Parameter Store (admin/prod not allowed). This is used to determine target mysql database names. These environments are not used for the s3 copy, which is declared in a separate arg',
                        default='staging'
                       )
    parser.add_argument('--dbhost',
                        help='FQDN of the mysql host we will be restoring to',
                        default='CHANGEME_RDS_STRING.ap-southeast-2.rds.amazonaws.com'
                       )
    parser.add_argument('--dbport',
                        help='Port of the mysql host we will be restoring to',
                        default=3306
                       )
    parser.add_argument('--source-asset-bucket',
                        help='Name of source asset S3 bucket to clone from',
                        default='lightcomet-assets-prod'
                       )
    parser.add_argument('--target-asset-buckets',
                        help='comma-linked list of target S3 buckets to clone to',
                        default='lightcomet-assets-stag'
                       )

    options = parser.parse_args()

    try:
        options.loglevel = getattr(logging, options.loglevel.upper())
    except:
        print("Loglevel %s unrecognised, setting to INFO" % options.loglevel)
        options.loglevel = 20


    try:
        res = urlparse(options.s3_loc, allow_fragments=False)
        options.bucket = res.netloc
        options.key = res.path.lstrip('/')
        if res.scheme != 's3':
            raise ValueError("Scheme '%s' not supported" % res.scheme)
    except Exception as e:
        print("ERROR: Unable to parse '%s', exiting: %s" % (options.s3_loc, e))
        sys.exit(17)

    if not options.database_prefix:
        print("ERROR: must have a database prefix")
        sys.exit(13)

    options.environments = options.environments.split(',')
    env_badprefixes = ['prod', 'admin']
    options.environments = [x for x in options.environments if not any(prefix in x for prefix in env_badprefixes)]


    log.debug(options)

    return options



def get_aws_region():
    """ stupid AWS region problem
    """

    r = requests.get("http://169.254.169.254/latest/dynamic/instance-identity/document")
    response_json = r.json()
    region = response_json.get('region')

    return region


def notify_slack(options, message):
    """ notify a slack channel with a message

        the 'slackclient' module doesn't differentiate when installing, but
        'imports' with different names, and has completely different workflows.

        this means we can't rely on any particular behaviour if we don't know
        we're on a recent slackclient sdk
    """

    slack_major = pkg_resources.get_distribution('slackclient').version[0]

    if int(slack_major) < 2:
        sc = SlackClient(options.slack_token)
        result = sc.api_call("chat.postMessage",
                             channel=options.slack_channel,
                             text=message
                            )
    else:
        client = slack.WebClient(options.slack_token)
        result = client.chat_postMessage(channel=options.slack_channel, text=message)

    if not result['ok']:
        log.error("Failed to send slack message: %s", result['error'])

def send_slack_alert(options, failed_dumps, total_dumps, engine='mysql'):
    """ alert Slack on failed dumps
    """

    options.slack_channel = ssm_fetch_value('/admin/lightcomet/nightly-clone/slack-notify-channel', region=options.region)
    options.slack_token = ssm_fetch_value('/admin/lightcomet/nightly-clone/slack-notify-token', region=options.region)

    message = "LightComet stag/test database clone script on '%s' failed to clone %d (of %d) stag/test dbs from %s - check logs for more details" % (os.uname().nodename, failed_dumps, total_dumps, options.s3_loc)

    log.debug('Sending slack message...')
    notify_slack(options, message)




def ssm_fetch_value(key, region=False):
    """ given a key, return the value from Parameter Store
    """

    if not region:
        region = get_aws_region()

    ssm = boto3.client('ssm', region_name=region)

    value = ssm.get_parameter(Name=key)['Parameter']['Value']

    return value



def list_sql_dumpfiles(bucket, key, age_limit=None):
    """ search an s3 location for files ending in .sql
        age_limit in days will exclude older items

        example obj record
        {'ETag': '"02ccfe7f93b7b9186ac9d881965f268e-1"',
         'Key': 'rds/mysql/RDSPREFIX.ap-southeast-2.rds.amazonaws.com/microservices_prod.sql',
         'LastModified': datetime.datetime(2019, 9, 2, 10, 17, 5, tzinfo=tzutc()),
         'Size': 54572,
         'StorageClass': 'STANDARD'},
    """

    s3 = boto3.client('s3')

    dumpfile_objs = []
    api_count = 0

    # link describes using continuation tokens (we want full objs for timestamp, not just keys, though)
    # https://alexwlchan.net/2017/07/listing-s3-keys/
    kwargs = {'Bucket': bucket, 'Prefix': key}
    log.debug('Getting dumpfile keys from bucket %s with prefix %s', bucket, key)
    while True:
        response = s3.list_objects_v2(**kwargs)
        try:
            objlist = [x for x in response['Contents'] if x['Key'].lower().endswith('.sql')]
        except KeyError:
            log.error("No sql dumps found at '%s' - is this prefix correct? Aborting.", key)
            sys.exit(38)

        dumpfile_objs.extend(objlist)

        try:
            kwargs['ContinuationToken'] = response['NextContinuationToken']
            api_count += 1
        except KeyError:
            break

    if api_count > 3:
        log.warning("Had to poll s3 %d times (~1000 keys per time). Please consider narrowing search with a tighter bucket URL (s3://bucket/dbs/foo instead of s3://bucket) - cheaper and faster", api_count)

    if age_limit:
        stale_date = datetime.now(timezone.utc) - timedelta(days=age_limit)
        dumpfile_keys = [x['Key'] for x in dumpfile_objs if x['LastModified'] > stale_date]
    else:
        dumpfile_keys = [x['Key'] for x in dumpfile_objs]

    return dumpfile_keys



def fetch_mysql_credentials(options, ssm_prefix):
    """ fetch db connection details from AWS parameter store
    """

    mysql_details = {}
    for item in ['user', 'pass']:
        try:
            ssm_key = '/'.join([ssm_prefix.rstrip('/'), item])
            log.debug('Fetching Parameter Store value for %s', ssm_key)
            mysql_details[item] = ssm_fetch_value(ssm_key, region=options.region)
        except Exception as e:
            log.error("Failed to fetch mysql credentials '%s' from AWS Parameter Store: %s", item, e)
            raise

    log.debug(mysql_details)

    return mysql_details



def check_mysql_version(mysql_details):
    """ log a warning if restore db is older than (current at time of writing)
        prod dbs
    """

    min_version = 8

    cnx = mysql.connector.connect(
        host=mysql_details['host'],
        port=mysql_details['port'],
        user=mysql_details['user'],
        passwd=mysql_details['pass']
        )
    cursor = cnx.cursor()
    cursor.execute('select version();')
    for row in cursor:
        semver = row[0]
        if int(semver[0]) < min_version:
            log.warning("Restore database is on MySQL %s, which is lower than some of our dbs (v8.0+)", semver)
    cnx.close()


def process_mysql_dumpfiles(options, tmpdir):
    """ get mysql dumpfiles and try to verify them by restoring them
    """

    failed_dumps = []
    attempted_dumps = 0
    excluded_dumps = 0
    dumpfile_keys = list_sql_dumpfiles(options.bucket, options.key, age_limit=int(options.max_age))
    dumpfile_keys = [x for x in dumpfile_keys if os.path.basename(x).startswith(options.database_prefix)]
    log.debug('Dumpfile s3 keys: %s', dumpfile_keys)

    s3r = boto3.resource('s3')
    bucket = s3r.Bucket(options.bucket)
    log.info('Fetching dumpfiles from %s', options.bucket)

    if os.path.isfile(options.exclusion_list):
        with open(options.exclusion_list, 'r') as f:
            content = f.readlines()
            exclusion_list = [x.strip() for x in content]
    else:
        exclusion_list = []

    rds_host_namespace = options.dbhost.split('.')[0]

    total_dumps = len(dumpfile_keys) * len(options.environments)

    for key in dumpfile_keys:
        dumpfile = os.path.basename(key)

        localdump = '/'.join([tmpdir.rstrip('/'), dumpfile])


        log.debug('Downloading %s to %s...', dumpfile, localdump)
        bucket.download_file(key, localdump)

        # source database dumps expected to be in format 'PREFIX_NAMESPACE_ENV.sql', where
        # prefix = lc_    # it's optional here for iteration, using paultest_ for the moment
        # env = _prod     # 4-char version of env
        original_dbname = dumpfile.replace('.sql', '')
        namespace = original_dbname.split(options.database_prefix)[-1]
        namespace = namespace[:-5] # remove _prod/_stag etc

        for env in options.environments:

            target_dbname = options.database_prefix + namespace + '_' + env[:4]

            if target_dbname in exclusion_list:
                log.info("Excluded target database '%s' (in exclusion list)", target_dbname)
                excluded_dumps += 1
                continue

            log.info('Cloning %s to %s...', dumpfile, target_dbname)

            if env.startswith('test'):
                ssm_env = 'staging'  # testing dbs are on the staging RDS host, so use the staging 'tree' in Parameter Store
            else:
                ssm_env = env

            ssm_prefix = '/' + '/'.join([ssm_env, 'rds', rds_host_namespace, target_dbname])

            log.debug('Fetching mysql details for %s %s from Parameter Store (%s)', env, namespace, ssm_prefix)
            try:
                mysql_details = fetch_mysql_credentials(options, ssm_prefix=ssm_prefix)
            except Exception as e:
                log.error('Failed to retrieve mysql credentials from Parameter Store for %s %s: e', env, namespace)
                failed_dumps.append('%s from %s' % (env, key))
                continue
            mysql_details['host'] = options.dbhost
            mysql_details['port'] = options.dbport
            log.debug('Mysql details for %s %s: %s', env, namespace, mysql_details)

            try:
                log.debug('Restoring %s to %s %s...', dumpfile, env, namespace)
                attempted_dumps += 1
                clone_mysql_dumpfile(options, target_dbname, localdump, mysql_details)
            except subprocess.CalledProcessError:
                # the mysql restore prints useful error logs. The python exception is just "returned nonzero"
                log.error("MySQL exited nonzero trying to restore '%s'; mysql should have printed the error above", dumpfile)
                failed_dumps.append('%s from %s' % (env, key))



    return total_dumps, attempted_dumps, failed_dumps, excluded_dumps



def clone_mysql_dumpfile(options, dbname, dumpfile, mysql_details):
    """ tries a db restore
    """

    if options.dryrun:
        log.info('(would have processed db %s with %s and %s)', dbname, dumpfile, mysql_details) #FIXME
        return

    log.debug('Dropping database for %s ', dbname)
    drop_mysql_db(mysql_details, dbname)

    # https://stackoverflow.com/a/28266325/4719191 - "low memory" method
    with open(dumpfile, 'r') as df:

        # supply password as env var to avoid 'omg! cli passwd!' spam
        command = ['mysql',
                   '-h', mysql_details['host'],
                   '-P', str(mysql_details['port']),
                   '-u', mysql_details['user'],
                   dbname
                  ]
                   #'-p' + mysql_details['pass'],

        log.debug('Restoring dumpfile...')
        subprocess.check_call(command, stdin=df, env=dict(os.environ, MYSQL_PWD=mysql_details['pass']))



def drop_mysql_db(mysql_details, dbname):
    """ destroys the db and creates an empty one
        needs a user with perms, naturally
    """


    cnx = mysql.connector.connect(
        host=mysql_details['host'],
        port=mysql_details['port'],
        user=mysql_details['user'],
        passwd=mysql_details['pass']
        )

    cursor = cnx.cursor()

    # can't string these two sql commands together with multi=True;
    # if the first fails (eg: no pre-existing db), the second is not run
    log.debug("Dropping db '%s'...", dbname)
    sql = "DROP DATABASE IF EXISTS %s;" % dbname
    cursor.execute(sql)
    log.debug("Creating db '%s' again...", dbname)
    sql = "CREATE DATABASE %s;" % dbname
    cursor.execute(sql)

    cnx.close()




# pinched from our s3 bucket backup script
def stream_command(command):
    """ will print output of subshell to STDOUT in realtime
        nicked from https://zaiste.net/realtime_output_from_shell_command_in_python/

        needs to be called in a loop like:
        for line in stream_command("ping google.com"):
            log.info(line)
    """

    log.debug("about to stream output for command '%s'", command)
    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)
    while True:
        line = process.stdout.readline().rstrip()
        if not line:
            break
        yield line

    process.poll()

    rc = process.returncode
    if rc != 0:
        log.error("Command failed with return code %d", rc)
        log.error("command: %s", command)
        raise ValueError


def s3_bucket_sync(source_loc, backup_loc, dryrun=False, verbose=False):
    """ copy source to backup using aws s3 sync as a subprocess
    """

    # cron needs full paths
    # '--exact-timestamps' is used because sometimes s3 sync doesn't sync newer timestamps if same size
    cmdprefix = '/usr/local/bin/aws s3 sync --exact-timestamps'

    if dryrun == True:
        cmdprefix += ' --dryrun'

    if verbose == False:
        cmdprefix += ' --only-show-errors'

    cmd = ' '.join([cmdprefix, source_loc, backup_loc])
    log.debug(cmd)

    issues = False
    try:
        for line in stream_command(cmd):
            log.info(' %s', line)
    except Exception as e:
        log.error("Unexpected error when syncing %s (logs should be above ^^): %s", source_loc, e)
        issues = "Issue syncing %s to %s" % (source_loc, backup_loc)

    return issues

def clone_s3_buckets(options):
    """ clone... s3... buckets?
    """

    log.debug('Start cloning s3 buckets')

    source_loc = 's3://' + options.source_asset_bucket + '/'

    problems = []
    for target_bucket in options.target_asset_buckets.split(','):
        target_loc = 's3://' + target_bucket + '/'

        log.info('Cloning s3 buckets %s to %s...', source_loc, target_loc)

        issues = s3_bucket_sync(source_loc, target_loc, dryrun=options.dryrun)

        if issues:
            problems.append(issues)


    if problems:
        log.error("Summary of problem syncs during this backup run:")
        for issue in problems:
            log.error("- %s", issue)

        # There isn't an easy way to get the AWS cli tool error into this message...
        notify_slack(options,
                     "Lightcomet S3 clone on %s encountered problems, see script logs (in syslog) for details. Problem summary: %s" % (os.uname().nodename, '; '.join(problems)),
                     level='error')



def main():
    """ ... main?
    """

    options = getoptions()

    formatter = logging.Formatter('%(levelname)s: %(message)s')
    log_handler = logging.StreamHandler()
    log_handler.setLevel(options.loglevel)
    log_handler.setFormatter(formatter)
    log.addHandler(log_handler)

    log.info("Start cloning Lightcomet databases and s3 asset buckets...")


    mysql_start_time = time.time()

    # this should auto-cleanup the directory after it's all done
    with tempfile.TemporaryDirectory() as tmpdir:
        log.debug("Temp dir: %s", tmpdir)

        mysql_total_dumps, mysql_attempted_dumps, mysql_failed_dumplist, mysql_excluded_dumps = process_mysql_dumpfiles(options, tmpdir)

        mysql_numfailed = len(mysql_failed_dumplist)

    mysql_end_time = time.time()
    mysql_validation_seconds = int(mysql_end_time - mysql_start_time)

    if not options.suppress_notifications:

        ## Only send a notification on failure
        if mysql_numfailed > 0:
            try:
                send_slack_alert(options,
                                 mysql_numfailed,
                                 mysql_total_dumps,
                                 engine='mysql'
                                )
            except Exception as e:
                log.error('Failed to send alert to slack: %s', e)



    if mysql_numfailed > 0:
        log.error('S3 keys (dumpfiles) failing sql restore/validation:\n - %s',
                  '\n - '.join(mysql_failed_dumplist)
                 )


    clone_s3_buckets(options)


    log.info("Mysql and S3 clone attempts complete.")
    log.info("%d/%d/%d/%d total/attempted/failed/excluded mysql restores (took %s)",
             mysql_total_dumps,
             mysql_attempted_dumps,
             mysql_numfailed,
             mysql_excluded_dumps,
             str(timedelta(seconds=mysql_validation_seconds))
            )
    log.info('Done.')



if __name__ == '__main__':
    main()
